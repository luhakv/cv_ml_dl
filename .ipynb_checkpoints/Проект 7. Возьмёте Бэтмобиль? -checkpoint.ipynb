{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Юнит 9. Профориентация в Data Science\n",
    "### Skillfactory: DSPR-19\n",
    "### Проект 7. Возьмёте Бэтмобиль? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Чем займёмся в этом проекте?\n",
    "→ Поздравляем! Вы приступаете к выполнению финального проекта блока Профориентация, в котором вы сможете на практике поработать с задачами из всех трёх направлений — ML, CV и NLP.\n",
    "\n",
    "За основу возьмем решение, которое нам уже известно, — построенную на табличных данных модель «Предсказание стоимости автомобиля» — и добавим туда модели Deep Learning, которые будут распознавать изображения и обрабатывать текст.\n",
    "\n",
    "За последние несколько месяцев работы в компании «Старый друг» вы разработали две модели, которые уже успешно крутятся в продакшне и приносят деньги компании и радость сотрудникам (по словам Анны из отдела продаж).\n",
    "\n",
    "→ Осталось довести свою работу до логичного завершения и, обогатив датасет текстовыми данными из объявлений о продаже, свести все наши модели в единое решение — Multi-inputs сеть. Приступим?\n",
    "\n",
    "### КАК БУДЕМ РЕШАТЬ ПРОЕКТ?\n",
    "\n",
    "### ПОСТРОИМ БАЗОВОЕ РЕШЕНИЕ:\n",
    "\n",
    "- Шаг 1. Построим «наивную» ML-модель на табличных данных, которая предсказывает цену по модели и году выпуска. Это позволит нам задать направление для дальнейших экспериментов. Впоследствии на этом этапе вы можете доработать вашу модель из проекта «Выбираем авто выгодно». \n",
    "- Шаг 2. Обработаем и отнормируем признаки и сделаем первую модель на основе градиентного бустинга с помощью CatBoost.\n",
    "- Шаг 3. Решим эту же задачу с помощью DL (модель NN Tabular) и сравним результаты.\n",
    "- Шаг 4. Добавим текстовые данные (NLP) и сделаем Multi-Input нейронную сеть для анализа и табличных данных, и текста одновременно.\n",
    "- Шаг 5. Добавим обработку изображений в Multi-Input нейронную сеть.\n",
    "- Шаг 6. Осуществим ансамблирование градиентного бустинга и нейронной сети (усредним их предсказания).\n",
    "\n",
    "### САМОСТОЯТЕЛЬНАЯ ЧАСТЬ\n",
    "\n",
    "- Ознакомьтесь с критериями оценивания проекта.\n",
    "- Следуйте нашим рекомендациям, чтобы построить самую лучшую модель.\n",
    "- Задавайте вопросы одногруппникам и менторам в Slack.\n",
    "- Загрузите итоговое решение на kaggle и добавьте описание проекта на git.  \n",
    "\n",
    "На выполнение проекта отводится: две недели.\n",
    "\n",
    "### ЧТО ВЫ ПОЛУЧИТЕ В РЕЗУЛЬТАТЕ ПРОЕКТА?\n",
    "\n",
    "- Потренируетесь обрабатывать данные и оптимизировать NLP-модели.\n",
    "- Пройдетесь по всему циклу разработки комплексной Multi-Inputs модели (от обработки данных до внедрения в продакшн).\n",
    "- Увидите, как использование blend-решений влияет на целевые метрики.\n",
    "- Примерите на себя роль специалиста каждого из трёх треков второго года обучения.\n",
    "\n",
    "\n",
    "\n",
    "— Возьмете Бэтмобиль?  \n",
    "— Нет. Слишком заметно.  \n",
    "— Ну тогда возьмите Ламборгини. Это менее заметно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Начнём с простой модели\n",
    "\n",
    "Начнём с разработки baseline нашего решения и посмотрим, как происходит обращение с входящими данными и что нужно получить на выходе. \n",
    "\n",
    "По ссылке вы обнаружите baseline. \n",
    "\n",
    "СКАЧАТЬ BASELINE (https://github.com/luhakv/cv_ml_nlp/blob/master/baseline-sf-dst-car-price-part2-v6.ipynb)\n",
    "\n",
    "Итак, наша задача — предсказать стоимость автомобиля по его характеристикам, текстовому описанию или картинке. \n",
    "\n",
    "Начнём знакомиться с baseline-решением. Как видим, данные не сильно отличаются от прежних:\n",
    "\n",
    "bodyType — категориальный  \n",
    "brand —  категориальный   \n",
    "color —  категориальный  \n",
    "description —  текстовый  \n",
    "engineDisplacement — числовой, представленный как текст  \n",
    "enginePower — числовой, представленный как текст  \n",
    "fuelType — категориальный  \n",
    "mileage —  числовой   \n",
    "modelDate — числовой  \n",
    "model_info — категориальный  \n",
    "name — категориальный, желательно сократить размерность  \n",
    "numberOfDoors — категориальный  \n",
    "price — числовой, целевой  \n",
    "productionDate — числовой  \n",
    "sell_id — изображение (файл доступен по адресу, основанному на sell_id)  \n",
    "vehicleConfiguration — не используется (комбинация других столбцов)  \n",
    "vehicleTransmission — категориальный  \n",
    "Владельцы — категориальный  \n",
    "Владение — числовой, представленный как текст  \n",
    "ПТС — категориальный  \n",
    "Привод — категориальный  \n",
    "Руль — категориальный  \n",
    "\n",
    "\n",
    "Однако кое-что, скорее всего, придётся переделать, поскольку есть дополнительные столбцы. \n",
    "\n",
    "→ Переходим к построению самой простой «наивной» модели, которая предсказывает среднюю цену по модели автомобиля и году выпуска. Начинать решение всегда стоит именно с простой модели, чтобы понимать, куда вы движетесь — к улучшению или к ухудшению решения. Это сэкономит вам множество сил и ресурсов. \n",
    "\n",
    "Наша модель предсказывает среднее значение по стоимости относительно модели и года выпуска.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наивная модель\n",
    "predicts = []\n",
    "for index, row in pd.DataFrame(data_test[['model_info', 'productionDate']]).iterrows():\n",
    "    query = f\"model_info == '{row[0]}' and productionDate == '{row[1]}'\"\n",
    "    predicts.append(data_train.query(query)['price'].median())\n",
    "\n",
    "# заполним не найденные совпадения\n",
    "predicts = pd.DataFrame(predicts)\n",
    "predicts = predicts.fillna(predicts.median())\n",
    "\n",
    "# округлим\n",
    "predicts = (predicts // 1000) * 1000\n",
    "\n",
    "#оцениваем точность\n",
    "print(f\"Точность наивной модели по метрике MAPE: {(mape(data_test['price'], predicts.values[:, 0]))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EDA и модель на основе градиентного бустинга\n",
    "\n",
    "На этом этапе нам важно посмотреть на распределение числовых признаков, опуская категориальные, поскольку задача этого шага — посмотреть, подходят ли данные для загрузки в сеть. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#посмотрим, как выглядят распределения числовых признаков\n",
    "def visualize_distributions(titles_values_dict):\n",
    "  columns = min(3, len(titles_values_dict))\n",
    "  rows = (len(titles_values_dict) - 1) // columns + 1\n",
    "  fig = plt.figure(figsize = (columns * 6, rows * 4))\n",
    "  for i, (title, values) in enumerate(titles_values_dict.items()):\n",
    "    hist, bins = np.histogram(values, bins = 20)\n",
    "    ax = fig.add_subplot(rows, columns, i + 1)\n",
    "    ax.bar(bins[:-1], hist, width = (bins[1] - bins[0]) * 0.7)\n",
    "    ax.set_title(title)\n",
    "  plt.show()\n",
    "\n",
    "visualize_distributions({\n",
    "    'mileage': train['mileage'].dropna(),\n",
    "    'modelDate': train['modelDate'].dropna(),\n",
    "    'productionDate': train['productionDate'].dropna()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение признаков ненормальное, поэтому их необходимо нормировать. \n",
    "\n",
    "Разбиваем на категориальные признаки и числовые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#используем все текстовые признаки как категориальные без предобработки\n",
    "categorical_features = ['bodyType', 'brand', 'color', 'engineDisplacement', 'enginePower', 'fuelType', 'model_info', 'name',\n",
    "  'numberOfDoors', 'vehicleTransmission', 'Владельцы', 'Владение', 'ПТС', 'Привод', 'Руль']\n",
    "\n",
    "#используем все числовые признаки\n",
    "numerical_features = ['mileage', 'modelDate', 'productionDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем тренировочный и тестовый сеты:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sample'] = 1 # помечаем где у нас трейн\n",
    "test['sample'] = 0 # помечаем где у нас тест\n",
    "test['price'] = 0 # в тесте у нас нет значения price, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = test.append(train, sort=False).reset_index(drop=True) # объединяем\n",
    "print(train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняем предобработку данных: удаляем лишнее, заполняем пропуски, если они есть, выполняем нормирование. На CatBoost это не должно повлиять. Наконец, выполняем Label Encoding или One-Hot Encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(df_input):\n",
    "    '''includes several functions to pre-process the predictor data.'''\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    # ################### 1. Предобработка ############################################################## \n",
    "    # убираем не нужные для модели признаки\n",
    "    df_output.drop(['description','sell_id',], axis = 1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # ################### Numerical Features ############################################################## \n",
    "    # Далее заполняем пропуски\n",
    "    for column in numerical_features:\n",
    "        df_output[column].fillna(df_output[column].median(), inplace=True)\n",
    "    # тут ваш код по обработке NAN\n",
    "    # ....\n",
    "    \n",
    "    # Нормализация данных\n",
    "    scaler = MinMaxScaler()\n",
    "    for column in numerical_features:\n",
    "        df_output[column] = scaler.fit_transform(df_output[[column]])[:,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ################### Categorical Features ############################################################## \n",
    "    # Label Encoding\n",
    "    for column in categorical_features:\n",
    "        df_output[column] = df_output[column].astype('category').cat.codes\n",
    "        \n",
    "    # One-Hot Encoding: в pandas есть готовая функция - get_dummies.\n",
    "    df_output = pd.get_dummies(df_output, columns=categorical_features, dummy_na=False)\n",
    "    # убираем признаки которые еще не успели обработать, \n",
    "    df_output.drop(['vehicleConfiguration'], axis = 1, inplace=True)\n",
    "    \n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем и смотрим на чистые данные. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc = preproc_data(data)\n",
    "df_preproc.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили целевую переменную price, дату производства, пробег, столбец с указанием принадлежности к тренировочному или тестовому наборам.   \n",
    "\n",
    "Затем выделяем тестовую часть:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data.price.values     # наш таргет\n",
    "X = train_data.drop(['price'], axis=1)\n",
    "X_sub = test_data.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем запустить уже не наивную модель, а построить модель на основе градиентного бустинга при помощи CatBoost. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations = 5000,\n",
    "                          #depth=10,\n",
    "                          #learning_rate = 0.5,\n",
    "                          random_seed = RANDOM_SEED,\n",
    "                          eval_metric='MAPE',\n",
    "                          custom_metric=['RMSE', 'MAE'],\n",
    "                          od_wait=500,\n",
    "                          #task_type='GPU',\n",
    "                         )\n",
    "model.fit(X_train, y_train,\n",
    "         eval_set=(X_test, y_test),\n",
    "         verbose_eval=100,\n",
    "         use_best_model=True,\n",
    "         #plot=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем MAPE у тестовой части: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_catboost = model.predict(X_test)\n",
    "print(f\"TEST mape: {(mape(y_test, test_predict_catboost))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили MAPE 13.23%. Это неплохой результат, поскольку CatBoost хорошо работает с небольшим количеством данных и множеством категориальных признаков.\n",
    "\n",
    "Сохраняем submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_predict_catboost = model.predict(X_sub)\n",
    "sample_submission['price'] = sub_predict_catboost\n",
    "sample_submission.to_csv('catboost_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Простая нейронная сеть\n",
    "\n",
    "Перед построением Dense-модели проверим данные и убедимся в том, что их формат нам подходит:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полторы тысячи столбцов — это большое количество, но нейронные сети с таким объёмом справляются спокойно. Далее строим обычную Dense-сеть из трёх слоёв с дропаутами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(L.Dropout(0.5))\n",
    "model.add(L.Dense(256, activation=\"relu\"))\n",
    "model.add(L.Dropout(0.5))\n",
    "model.add(L.Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компилируем модель:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем чекпойнты и настраиваем EarlyStopping:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/best_model.hdf5' , monitor=['val_MAPE'], verbose=0  , mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_MAPE', patience=50, restore_best_weights=True,)\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение на 500 эпох. batch_size устанавливаем достаточно большого размера, поскольку данные занимают немного места.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=0,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на нашу сеть:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['MAPE'], label='train')\n",
    "plt.plot(history.history['val_MAPE'], label='test')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кривая обучения получилась достаточно ровной.\n",
    "\n",
    "Не забываем сохранить:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../working/best_model.hdf5')\n",
    "model.save('../working/nn_1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И смотрим на MAPE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_nn1 = model.predict(X_test)\n",
    "print(f\"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, простая нейронная сеть нам дала 14%.\n",
    "\n",
    "Сохраняем submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_predict_nn1 = model.predict(X_sub)\n",
    "sample_submission['price'] = sub_predict_nn1[:,0]\n",
    "sample_submission.to_csv('nn1_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КАК МОЖНО УЛУЧШИТЬ ЭТУ СЕТЬ?\n",
    "\n",
    "1. В нейросеть желательно подавать данные с распределением, близким к нормальному, поэтому от некоторых числовых признаков имеет смысл перед нормализацией взять логарифм. \n",
    "\n",
    "Пример: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDateNorm = np.log(2020 - data['modelDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Статья по теме на Хабре (https://habr.com/ru/company/ods/blog/325422/). \n",
    "\n",
    "2. Извлечение числовых значений из текста. Парсинг признаков 'engineDisplacement', 'enginePower', 'Владение' для извлечения числовых значений.\n",
    "\n",
    "3. Cокращение размерности категориальных признаков. Признак 'name' содержит данные, которые уже есть в других столбцах ('enginePower', 'engineDisplacement', 'vehicleTransmission'). Можно удалить эти данные. Затем можно ещё сильнее сократить размерность, например, выделив наличие xDrive в качестве отдельного признака.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-Input сеть: табличные данные + текст\n",
    "\n",
    "Вспомните, что вы узнали в модуле об NLP и обработке текста. Итак, у нас есть текстовое описание: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведём токенизацию этого поля:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_WORDS = 100000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И не забываем разбить данные: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split данных\n",
    "text_train = data.description.iloc[X_train.index]\n",
    "text_test = data.description.iloc[X_test.index]\n",
    "text_sub = data.description.iloc[X_sub.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём словарь — в Keras это занимает буквально пару строк:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenize = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenize.fit_on_texts(data.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря получившемуся словарю мы сможем произвести дальнейшую векторизацию нашего текста. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_train_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "text_test_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "text_sub_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(text_train_sequences.shape, text_test_sequences.shape, text_sub_sequences.shape, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как теперь выглядит наш текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_train.iloc[6])\n",
    "print(text_train_sequences[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь строим сеть для обработки текста. Для простоты в примере используем LSTM:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nlp = Sequential()\n",
    "model_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"))\n",
    "model_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))\n",
    "model_nlp.add(L.LSTM(256, return_sequences=True))\n",
    "model_nlp.add(L.Dropout(0.5))\n",
    "model_nlp.add(L.LSTM(128,))\n",
    "model_nlp.add(L.Dropout(0.25))\n",
    "model_nlp.add(L.Dense(64, activation=\"relu\"))\n",
    "model_nlp.add(L.Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что эта сеть построена «без головы». Добавим ещё одну сеть — созданную нами ранее для табличных данных:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = Sequential()\n",
    "model_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model_mlp.add(L.Dropout(0.5))\n",
    "model_mlp.add(L.Dense(256, activation=\"relu\"))\n",
    "model_mlp.add(L.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам осталось объединить их в Multi-Input сеть, то есть сеть, которая позволяет брать на вход несколько сетей и объединять их результаты. За объединение отвечает слой L.concatenate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = L.concatenate([model_nlp.output, model_mlp.output])\n",
    "# being our regression head\n",
    "head = L.Dense(64, activation=\"relu\")(combinedInput)\n",
    "head = L.Dense(1, activation=\"linear\")(head)\n",
    "\n",
    "model = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, существуют и сети с несколькими выходами, но наша задача на этом этапе — отработать построение сети с несколькими входами и одним выходом.\n",
    "\n",
    "Смотрим, что у нас получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовимся к обучению: настраиваем показатели, чекпойнты, EarlyStopping:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_MAPE', patience=10, restore_best_weights=True,)\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важный момент при обучении — указать отдельно текстовые элементы и табличные, а для валидации не забыть указать несколько входов именно в той последовательности, которую мы установили ранее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([text_train_sequences, X_train], y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n",
    "                    validation_data=([text_test_sequences, X_test], y_test),\n",
    "                    callbacks=callbacks_list\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как прошло обучение:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['MAPE'], label='train')\n",
    "plt.plot(history.history['val_MAPE'], label='test')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забываем сохраняться. Затем смотрим на MAPE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_nn2 = model.predict([text_test_sequences, X_test])\n",
    "print(f\"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что показатель ошибки стал чуть выше. Скорее всего, это связано с недостаточной обработкой текста перед реализацией модели.\n",
    "\n",
    "Не забудьте сохранить submission!\n",
    "\n",
    "### КАК МОЖНО УЛУЧШИТЬ ЭТУ МОДЕЛЬ?\n",
    "\n",
    "Выделить из описаний часто встречающиеся блоки текста, заменив их на кодовые слова или удалив их.\n",
    "Сделать предобработку текста, например лемматизацию — алгоритм, который ставит все слова в форму по умолчанию (глаголы в инфинитив и так далее). Это позволит токенайзеру не преобразовывать разные формы слова в разные числа. Статья по теме на Хабре (https://habr.com/ru/company/Voximplant/blog/446738/).\n",
    "Поработать над алгоритмами очистки и аугментации текста.  \n",
    "\n",
    "→ Мы закончили работать с NLP, познакомились с Multi-Input сетью — самое время перейти к картинкам!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
