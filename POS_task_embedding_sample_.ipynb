{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"POS_task_embedding.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsP7cV1_arhs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torchtext import datasets\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import random\n",
        "\n",
        "from gensim.models import FastText\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNRfW260a3jg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyk3BCHza5ru"
      },
      "source": [
        "train_data, _, test_data = datasets.UDPOS()\n",
        "train_data = [d for d in train_data]\n",
        "test_data = [d for d in test_data]\n",
        "\n",
        "train_tokens = [ [w.lower() for w in d[0]] for d in train_data]\n",
        "train_tags = [ d[1] for d in train_data]\n",
        "\n",
        "test_tokens = [[w.lower() for w in d[0]] for d in test_data]\n",
        "test_tags = [d[1] for d in test_data]\n",
        "\n",
        "tag2num = { t:i for i, t in enumerate(np.unique([tag for tags in train_tags for tag in tags])) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBDDAUdhbxCq"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "word_to_ix = {}\n",
        "for tokens in train_tokens:\n",
        "    for word in tokens:\n",
        "        word = stemmer.stem(word)\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_to_ix[\"UNK\"] =  len(word_to_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hkWDYVab5PT"
      },
      "source": [
        "max_len = 20\n",
        "pad_inds = len(tag2num)\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(w) for w in seq]\n",
        "    idxs = [to_ix[w] if w in to_ix else to_ix[\"UNK\"] for w in stemmed_words ]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "def prepare_data_for_inner_embeddings(all_tokens, all_tags, word_to_ix, tag2num, max_len, pad_tags):\n",
        "    all_tags = [np.array([tag2num[tag]  for tag in tags]) for tags in all_tags]\n",
        "    \n",
        "    all_tokens = [tokens[:max_len] for tokens in all_tokens]\n",
        "    all_tags = [tags[:max_len] for tags in all_tags]\n",
        "    \n",
        "    all_ids = []\n",
        "    for tokens in all_tokens:\n",
        "        ids = prepare_sequence(tokens, word_to_ix)\n",
        "        all_ids.append(ids)\n",
        "        \n",
        "    X_vecs = []\n",
        "    Y_vecs = []\n",
        "\n",
        "    for ids, tags in zip(all_ids, all_tags):\n",
        "        X_vecs.append(torch.tensor(ids, dtype=torch.long))\n",
        "        Y_vecs.append(torch.tensor(tags, dtype=torch.long))\n",
        "        \n",
        "    # в качестве заполнителя X используем новый индекс len(word_to_ix)\n",
        "    X = pad_sequence(X_vecs, batch_first=True, padding_value=len(word_to_ix))\n",
        "\n",
        "    # в качестве заполнителя Y используем pad_tags\n",
        "    Y = pad_sequence(Y_vecs, batch_first=True, padding_value=pad_tags)\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = prepare_data_for_inner_embeddings(train_tokens, train_tags, word_to_ix, tag2num, max_len, pad_inds)\n",
        "\n",
        "X_train.size(), Y_train.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNHAQxZcHhS"
      },
      "source": [
        "X_test, Y_test = prepare_data_for_inner_embeddings(test_tokens, test_tags, word_to_ix, tag2num, max_len, pad_inds)\n",
        "\n",
        "X_test.size(), Y_test.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeBb8AbNcdX_"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfjixGEdceuN"
      },
      "source": [
        "print(Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8RSqbmOckWA"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "bs = 128\n",
        "data = TensorDataset(X_train, Y_train)\n",
        "dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5lgGR7Dcl0X"
      },
      "source": [
        "class BiLSTMPOSTagger(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # padding_idx=pad_idx - это номер id \"заполнителя\". \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        raise NotImplementedException()       \n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHtLZSe8dBWV"
      },
      "source": [
        "def train_on_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input, b_tags = batch\n",
        "        \n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input)  \n",
        "\n",
        "        # outputs = [batch size, sent len, out dim]\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])       \n",
        "        # outputs = [batch size * sent len, out dim]\n",
        "\n",
        "        # b_tags = [batch size, sent len]\n",
        "        b_tags = b_tags.view(-1)\n",
        "        # b_tags = [batch size * sent len]\n",
        "        \n",
        "        loss = criterion(outputs, b_tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def predict_on_dataloader(model, dataloaded):\n",
        "    model.eval()\n",
        "        \n",
        "    all_outputs = []\n",
        "    all_tags = []\n",
        "    for batch in dataloaded:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input, b_tags = batch\n",
        "        outputs = model(b_input)  \n",
        "        \n",
        "        outputs = outputs.view(-1, outputs.shape[-1])       \n",
        "        b_tags = b_tags.view(-1)\n",
        "\n",
        "        all_outputs.append(outputs)\n",
        "        all_tags.append(b_tags)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs)\n",
        "    all_tags = torch.cat(all_tags)\n",
        "    \n",
        "    return all_outputs, all_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9HPx3ywdDUj"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uDf2IvsdEeg"
      },
      "source": [
        "INPUT_DIM = len(word_to_ix)+1\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(tag2num)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = len(word_to_ix)\n",
        "\n",
        "model = BiLSTMPOSTagger(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_inds)\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWu61Ea-dKbc"
      },
      "source": [
        "epochs = 50\n",
        "for e in range(epochs):\n",
        "    train_on_epoch(model, dataloader, optimizer)    \n",
        "    \n",
        "    all_outputs, all_tags = predict_on_dataloader(model, dataloader)\n",
        "    loss = criterion(all_outputs, all_tags).item()\n",
        "    all_outputs = all_outputs.detach().cpu().numpy()\n",
        "    all_tags = all_tags.detach().cpu().numpy()\n",
        "    \n",
        "    mask = all_tags != pad_inds\n",
        "    loss = loss/len(all_tags[mask]) \n",
        "    all_tags = all_tags[mask]\n",
        "    all_preds = np.argmax(all_outputs, axis=1)[mask]\n",
        "    \n",
        "    print(f\"{e}:\\tLoss {loss}, \"\n",
        "          f\"accuracy: {accuracy_score(all_tags, all_preds)}, \"\n",
        "          f\"f1-macro: {f1_score(all_tags, all_preds, average='macro')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kcXubKVdVov"
      },
      "source": [
        "def count_metrics(model, dataloader):\n",
        "  y_pred, y_true = predict_on_dataloader(model, dataloader)\n",
        "\n",
        "  y_pred = y_pred.detach().cpu().numpy()\n",
        "  y_true = y_true.detach().cpu().numpy()\n",
        "\n",
        "  mask = y_true != pad_inds\n",
        "  y_true = y_true[mask]\n",
        "  y_pred = np.argmax(y_pred, axis=1)[mask]\n",
        "\n",
        "  print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25kR8EZsdavw"
      },
      "source": [
        "count_metrics(model, dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAOUiigWdY7m"
      },
      "source": [
        "data = TensorDataset(X_test, Y_test)\n",
        "test_dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=bs)\n",
        "count_metrics(model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W32kZg1qdbSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}